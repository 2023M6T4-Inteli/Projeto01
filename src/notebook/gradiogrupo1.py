# -*- coding: utf-8 -*-
"""Copy of GradioGrupo1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wHMpsUIQa2H9r2F5OEMoNNGuaiiHY_c4

# 1.0 Setup

Instalação das dependencias nescesarias:
"""

# Instala a biblioteca gradio, que permite criar interfaces de usuário para protótipos de modelos de aprendizado de máquina.
# !pip install gradio

# Instala a biblioteca sentence-transformers, usada para sentence embedding
# !pip install sentence-transformers

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

import locale
locale.getpreferredencoding = lambda: 'UTF-8'

"""Instalação das bibliotecas utilizadas:"""

# Importa o módulo pickle, que permite serializar e desserializar objetos Python.
import pickle

# Importa o pandas, uma biblioteca de manipulação de dados popular.
import pandas as pd

# Importa o numpy, uma biblioteca para operações matemáticas e de matriz.
import numpy as np

# Importa o SentenceTransformer da biblioteca sentence-transformers. Isso será usado para converter sentenças em vetores.
from sentence_transformers import SentenceTransformer

# Importa o módulo Tokenizer da biblioteca Keras. Este módulo é usado para converter sentenças em sequências de números.
from keras.preprocessing.text import Tokenizer

# Importa a biblioteca gradio, que será usada para criar a interface do usuário.
import gradio as gr

# Importa a biblioteca matplotlib.pyplot, usada para a criação de gráficos e visualizações.
import matplotlib.pyplot as plt

# Instalação das dependencias PLN
!pip install unidecode
!pip install -U spacy
!python -m spacy download pt_core_news_sm

#Bibliotecas de PLN:

from unidecode import unidecode
import re
import spacy
from spacy.lang.pt.stop_words import STOP_WORDS
nlp = spacy.load('pt_core_news_sm')

#Biblioteca nuvem de palavras:
from wordcloud import WordCloud
import tempfile

"""# 2.0 Deploy do modelo"""

#importando o modelo
modeloTransformersImportado = ('/content/modelo_rede_neural_Transformers.pkl')

with open(modeloTransformersImportado, 'rb') as f:
    modeloTransformers = pickle.load(f)

#Carregando o vetorizador
vetorizador = SentenceTransformer('distiluse-base-multilingual-cased')

"""# 3.0 Teste do modelo importado"""

# função para prever os sentimentos
def prever1(dados):
  dados = vetorizador.encode(dados.tolist())
  dados = pd.DataFrame(dados)
  previsoes = modeloTransformers.predict(dados)
  previsoes = np.argmax(previsoes, axis=1)
  return previsoes

# Criação dos dados do dashboard referente à analize das palavras mais frequentes
def testar_frase(frase):
    dadosteste = {
        'frase': [frase],
    }
    dfteste = pd.DataFrame(dadosteste)
    sentimento_codificado = prever1(dfteste["frase"])
    if sentimento_codificado == 0:
      return "Positivo"
    elif sentimento_codificado == 1:
      return "Neutro"
    else:
      return "Negativo"

"""# 4.0 Pipeline

Vale mencionar que os tratamentos definidos aqui estão documentados e registrados no notebook de treinamento, eles estão aqui apenas para funcionamento do dashboard
"""

#Código para remoção dos acentos
def remocaoAcentos(dataframe):
    dataframe = dataframe.applymap(lambda x: unidecode(str(x)))
    return dataframe

def tratamentoMaiusculas(dataframe):
    # Pré-processamento das letras maiúsculas
    dataframe = dataframe.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    # Exibição dos dados pré-processados
    return dataframe

def tratamentoAbreviacoes(coluna):
    coluna = coluna.astype(str)
    coluna = coluna.str.lower()

    # Dicionário com as abreviações e suas expansões, incluindo as palavras a serem substituídas
    abreviacoes = {
        r'\bvc\b': 'você',
        r'\btbm\b': 'também',
        r'\bpq\b': 'porque',
        r'\bmt\b': 'muito',
        r'\bmto\b': 'muito',
        r'\bblz\b': 'beleza',
        r'\bjah\b': 'já',
        r'\bbj\b': 'beijo',
        r'\bflw\b': 'falou',
        r'\bvlw\b': 'valeu',
        r'\bkrl\b': 'caralho',
        r'\bqnd\b': 'quando',
        r'\bfds\b': 'fim de semana',
        r'\bqq\b': 'qualquer',
        r'\bmsg\b': 'mensagem',
        r'\bpra\b': 'para',
        r'\bp\b': 'para',
        r'\bpqp\b': 'puta que pariu',
        r'\bvlh\b': 'velho',
        r'\bnss\b': 'nossa',
        r'\bngm\b': 'ninguém',
        r'\bqm\b': 'quem',
        r'\bobs\b': 'observação',
        r'\bqt\b': 'quanto',
        r'\btbm ñ\b': 'também não',
        r'\bpf\b': 'por favor',
        r'\bpdc\b': 'pode crer',
        r'\btalkei\b': 'ok',
        r'\bd+\b': 'demais',
        r'\bdms\b': 'demais',
        r'\bsqn\b': 'só que não',
        r'\bmsm\b': 'mesmo',
        r'\bn/ao\b': 'não',
        r'\bn/\b': 'não',
        r'\bblz\b': 'beleza',
        r'\bq\b': 'que',
        r'\btd\b': 'tudo',
        r'\bpfvr\b': 'por favor',
        r'\bgnt\b': 'gente',
        r'\bto\b': 'estou',
        r'\bvcs\b': 'vocês',
        r'\bn/oa\b': 'não',
        r'\bsla\b': 'sei lá',
        r'\bflws\b': 'falou',
        r'\bbjos\b': 'beijos',
        r'\bkd\b': 'cadê',
        r'\blgl\b': 'legal',
        r'\bnum\b': 'número',
        r'\bjá q\b': 'já que',
        r'\bmano\b': 'irmão',
        r'\bpke\b': 'porque',
        r'\bpr\b': 'para',
        r'\btpm\b': 'tensão pré-menstrual',
        r'\btá\b': 'está',
        r'\bqmto\b': 'muito',
        r'\bñ\b': 'não',
        r'\bvamu\b': 'vamos',
        r'\btmj\b': 'estamos juntos',
        r'[0-9]': '',
        r'@\w+': '',
        r'[^\w\s]': '',
        r'\bwww\.[^\s]*': '',
        r'\bola\b': '',
        r'\n': '',
        r'\bbtg\b': '',
        r'\bpactual\b': '',
        r'\bvocê\b': '',
        r'\bdia\b': '',
        r'\bjá\b': '',
        r'\bsão\b': '',
        r'\bsó\b': '',
        r'\blink\b': '',
        r'\br\b': '',
        r'\brepost\b': '',
        r'\baté\b': '',
        r'\bpaulo\b': '',
        r'\bbio\b': '',
        r'\bdia\b': '',
        r'\bdias\b': '',
        r'\bsera\b': '',
        r'\blá\b': '',
        r'\bmês\b': '',
        r'\bestão\b': '',
        r'\balém\b': '',
        r'\bano\b': '',
        r'\banos\b': '',
    }

    # Aplica a substituição de cada abreviação no dataframe
    for abreviacao, expansao in abreviacoes.items():
        coluna = coluna.apply(lambda x: re.sub(abreviacao, expansao, x, flags=re.IGNORECASE))

    return coluna

stop_words = [
            '@', 'banco', 'btg', 'brg', 'pactual', 'btgpactual', 'pq', 'q', 'pra', 'vcs', 'vc', 'i', 'p', 'kkk', 'y', 'of',
            'n', 'a', 'à', 'as', 'o', 'os', 'e', 'aos', 'do', 'das', 'dos', 'das', 'de', 'deles', 'dela', 'deles', 'delas',
            'para', 'que', 'em', 'algo', 'algum', 'alguma', 'alguns', 'algumas', 'aqui', 'aquele', 'aquela', 'aqueles',
            'aquelas', 'aqui', 'aquilo', 'cá', 'com', 'como', 'cada', 'coisa', 'daquele', 'daquela', 'daquilo', 'daqueles',
            'daquelas', 'desse', 'deste', 'dessa', 'desses', 'destes', 'destas', 'ele', 'eles', 'ela', 'elas', 'eu', 'nos',
            'nós', 'vocês', 'voces', 'enquanto', 'era', 'está', 'estamos', 'estão', 'estar', 'estará', 'estive', 'estivemos',
            'estiver', 'estivera', 'estiveram', 'estivéramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem',
            'estivéssemos', 'estiveste', 'estivestes', 'estou', 'fará', 'farta', 'farto', 'fez', 'fim', 'foi', 'fomos',
            'for', 'fora', 'foram', 'fôramos', 'forem', 'formos', 'fosse', 'fossem', 'fôssemos', 'foste', 'fostes', 'fui',
            'fôssemos', 'há', 'houve', 'hoje', 'isso', 'isto', 'já', 'lá', 'lhe', 'lhes', 'lo', 'logo', 'mas', 'me', 'mesma',
            'mesmas', 'mesmo', 'mesmos', 'meu', 'meus', 'minha', 'minhas', 'na', 'no', 'nas', 'nos', 'naquela', 'naquelas',
            'naquele', 'naqueles', 'nem', 'nessa', 'nessas', 'nesse', 'nesses', 'nesta', 'nestas', 'neste', 'nestes',
            'ninguém', 'nosso', 'nossa', 'nossos', 'nossas', 'num', 'numa', 'outra', 'outras', 'outro', 'outros', 'pela',
            'pelo', 'perante', 'pois', 'ponto', 'pontos', 'por', 'porém', 'porque', 'porquê', 'própria', 'próprio',
            'próprias', 'próprios', 'qual', 'quando', 'quanto', 'quantos', 'quantas', 'quê', 'quem', 'quer', 'quereis',
            'querem', 'queremas', 'quis', 'quisemos', 'quiser', 'quisera', 'quiseram', 'quiséramos', 'quiserem',
            'quisermos', 'quisésseis', 'quiséssemos', 'quiseste', 'quisestes', 'quiseste', 'quisestes', 'quizer',
            'quizeram', 'quizerem', 'quizermos', 'quizesse', 'quizessem', 'quizéssemos', 'são', 'se', 'seja', 'sejam',
            'sejamos', 'sem', 'sendo', 'ser', 'será', 'serão', 'será', 'seriam', 'seríamos', 'serias', 'seríeis', 'sete',
            'seu', 'seus', 'sob', 'sobre', 'sois', 'só', 'somos', 'sou', 'sua', 'suas', 'tal', 'talvez', 'também', 'te',
            'tem', 'têm', 'temos', 'tendes', 'tenha', 'tenham', 'tenhamos', 'tenho', 'tens', 'ter', 'terá', 'terão',
            'terá', 'teriam', 'teríamos', 'terias', 'teríeis', 'teu', 'teus', 'teve', 'tivemos', 'tiver', 'tivera',
            'tiveram', 'tivéramos', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéssemos', 'tiveste', 'tivestes',
            'tiveste', 'tivestes', 'um', 'uma', 'umas', 'uns'
        ]

for word in stop_words:
    nlp.vocab[word].is_stop = True

def remocaoStopWords(coluna):
    listaSemStopWords = []
    for texto in coluna:
        # converte o texto em um documento do Spacy
        doc = nlp(texto)
        # filtra os tokens que não são stop words
        tokenSemStopwords = [token.text for token in doc if not token.is_stop]
        # adiciona a lista de frases sem stop words
        listaSemStopWords.append(' '.join(tokenSemStopwords))
    # substitui a coluna original pelos textos sem stop words
    coluna[:] = listaSemStopWords
    return coluna

def pipeline(dados):
  #cria um novo df a ser tratado
  dadosT =  pd.DataFrame({'dadosTratados': dados})
  # substituição das letras maiusculas dos dados
  dadosT['dadosTratados'] = tratamentoMaiusculas(dadosT)
  #tratamento de stopwords
  dadosT['dadosTratados'] = remocaoStopWords(dadosT['dadosTratados'])
  #tratamento de abreviações
  dadosT['dadosTratados'] = tratamentoAbreviacoes(dadosT['dadosTratados'])
  # remove os acentos dos dados
  dadosT['dadosTratados'] = remocaoAcentos(dadosT)
  return dadosT['dadosTratados']

"""# 5.0 Contagem de palavras"""

#Definição de função para criação de dicionario
def tokenizacaoPalavra(comentarios):
  tokenizer = Tokenizer()
  tokenizer.fit_on_texts(comentarios)
  return tokenizer

#Definição de função para contagem de palavras
def contarPalavrasFrequentes(tokenizer, top_n):
    contagem_palavras = tokenizer.word_counts
    palavras_frequentes = dict(sorted(contagem_palavras.items(), key=lambda item: item[1], reverse=True)[:top_n])
    return palavras_frequentes

# Função para calcular as palavras mais frequentes
def frequentes(dados):
  dic = tokenizacaoPalavra(dados)
  palavras_frequentes = contarPalavrasFrequentes(dic, 15)

  return palavras_frequentes

"""## 5.1 Grafico com palavras mais frequentes"""

# definição de função para gerar uma grafico com as palavras mais frequentes
def gerarGraficoFrequencia(frequencia):
  # Cria um DataFrame a partir do dicionário de frequência
  df_frequencia = pd.DataFrame.from_dict(frequencia, orient='index', columns=['Contagem'])

  # Ordena o DataFrame pela contagem em ordem decrescente
  df_frequencia = pd.DataFrame.from_dict(frequencia, orient='index', columns=['Contagem'])

  # Ordena o DataFrame pela contagem em ordem decrescente
  df_frequencia = df_frequencia.sort_values('Contagem', ascending=False)

  # Cria o gráfico de barras
  plt.figure(figsize=(10, 6))
  plt.bar(df_frequencia.index, df_frequencia['Contagem'])
  plt.xlabel('Palavra')
  plt.ylabel('Contagem')
  plt.title('Frequência das Palavras')
  plt.xticks(rotation=45)
  plt.tight_layout()
  # Armazena o gráfico de barras na variável graficofrequencia
  graficofrequencia = plt.gcf()
  return graficofrequencia

"""# 6.0 Sentimentos frequentes"""

#função para definir o sentimento gerado pelas palavras mais frequentes
def sentimentoFrequentes(dados):
  sentimentoMaisFrequentes = frequentes(dados)
  sentimentoMaisFrequentes = list(sentimentoMaisFrequentes.keys())
  sentimentoMaisFrequentes = pd.DataFrame({'palavra': sentimentoMaisFrequentes})
  sentimentoM = prever1(sentimentoMaisFrequentes['palavra'])
  sentimentoMaisFrequentes['previsto'] = sentimentoM
  return sentimentoMaisFrequentes

"""# 7.0 Nuvem de palavras"""

def gerarNuvemPalavras(dados):

  dic = tokenizacaoPalavra(dados)
  dicionarioFreq = contarPalavrasFrequentes(dic, 200)
  # Cria uma instância da classe WordCloud com as configurações desejadas
  nuvemPalavras = WordCloud(width=3200, height=1600, background_color='white', colormap='viridis')

  # Gera a nuvem de palavras a partir do dicionário de frequências
  nuvemPalavras.generate_from_frequencies(dicionarioFreq)

  # Retorna a nuvem de palavras gerada
  return nuvemPalavras

def exibirNuvemPalavras(dicionarioFreq):
    nuvem = gerarNuvemPalavras(dicionarioFreq)

    # Salva a nuvem de palavras em um arquivo temporário
    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_file:
        arquivo_temporario = temp_file.name
        nuvem.to_file(temp_file.name)

    return arquivo_temporario

"""# 8.0 Grafico Sentimento - Mês"""

#Definindo função para gerar um grafico de sentimentos por tempo
def prepare_and_plot(dfData, _nome_da_coluna_datas):
    # Preparação dos dados
    dfData[_nome_da_coluna_datas] = dfData[_nome_da_coluna_datas].str.slice(1, 11)
    dfData[_nome_da_coluna_datas] = pd.to_datetime(dfData[_nome_da_coluna_datas])
    dfData['ano'] = dfData[_nome_da_coluna_datas].dt.year
    dfData['mes'] = dfData[_nome_da_coluna_datas].dt.month

    # Filtragem dos sentimentos
    df_negativos = dfData[dfData['sentimento'] == "NEGATIVE"]
    df_positivos = dfData[dfData['sentimento'] == "POSITIVE"]
    df_neutros = dfData[dfData['sentimento'] == "NEUTRAL"]

    # Contagem dos registros
    contagem_negativos_por_mes_ano = df_negativos.groupby(['ano','mes']).size().reset_index(name='quantidade_negativos')
    contagem_positivos_por_mes_ano = df_positivos.groupby(['ano','mes']).size().reset_index(name='quantidade_positivos')
    contagem_neutros_por_mes_ano = df_neutros.groupby(['ano','mes']).size().reset_index(name='quantidade_neutros')

    # Concatenação do ano e mês
    contagem_negativos_por_mes_ano['ano_mes'] = contagem_negativos_por_mes_ano['ano'].astype(str) + '-' + contagem_negativos_por_mes_ano['mes'].astype(str).str.zfill(2)
    contagem_positivos_por_mes_ano['ano_mes'] = contagem_positivos_por_mes_ano['ano'].astype(str) + '-' + contagem_positivos_por_mes_ano['mes'].astype(str).str.zfill(2)
    contagem_neutros_por_mes_ano['ano_mes'] = contagem_neutros_por_mes_ano['ano'].astype(str) + '-' + contagem_neutros_por_mes_ano['mes'].astype(str).str.zfill(2)

    # Geração do gráfico
    fig, ax = plt.subplots(figsize=(15,7))
    fig.patch.set_facecolor('#1F2937') #mudar a cor de fundo do grafico

    ax.plot(contagem_neutros_por_mes_ano['ano_mes'], contagem_neutros_por_mes_ano['quantidade_neutros'], linestyle='-', label='Neutro')
    ax.plot(contagem_negativos_por_mes_ano['ano_mes'], contagem_negativos_por_mes_ano['quantidade_negativos'], linestyle='-', label='Negativo')
    ax.plot(contagem_positivos_por_mes_ano['ano_mes'], contagem_positivos_por_mes_ano['quantidade_positivos'], linestyle='-', label='Positivo')

    ax.set_xlabel('Ano-Mes', color='white')
    ax.set_ylabel('Quantidade', color='white')
    ax.set_title('Gráfico de sentimentos em função do tempo', color='white')
    plt.xticks(rotation=90, color='white')
    plt.yticks(color='white')
    ax.legend()
    ax.grid(True)

    return fig

"""# 9.0 Cluster"""

def plot_clusters(labels, palavra_referencia):
    labels = tokenizacaoPalavra(labels)
    labels = list(labels.word_index.keys())

    vectorizer = TfidfVectorizer()
    words = vectorizer.fit_transform(labels)

    kmeans = KMeans(n_clusters=3, random_state=42)
    kmeans.fit(words)
    clusters = kmeans.labels_
    pca = PCA(n_components=2)
    reduced_features = pca.fit_transform(words.toarray())

    # Encontra o índice da palavra de referência
    if palavra_referencia in labels:
        indice_referencia = labels.index(palavra_referencia)
    else:
        print("Palavra de referência não encontrada.")
        return

    # Filtra as palavras próximas à palavra de referência
    indices_proximas = [indice for indice, _ in enumerate(labels) if abs(indice - indice_referencia) <= 3]
    labels_proximas = [labels[indice] for indice in indices_proximas]
    reduced_features_proximas = reduced_features[indices_proximas]
    clusters_proximas = clusters[indices_proximas]

    plt.figure(figsize=(10, 6))
    plt.scatter(reduced_features_proximas[:, 0], reduced_features_proximas[:, 1], c=clusters_proximas)

    for i, label in enumerate(labels_proximas):
        plt.annotate(label, xy=(reduced_features_proximas[i, 0], reduced_features_proximas[i, 1]), textcoords='data')

    plt.title('Clusterização de Palavras Próximas a "{}"'.format(palavra_referencia))
    plt.xlabel('Componente Principal 1')
    plt.ylabel('Componente Principal 2')
    plt.show()

    # Salva o gráfico em um arquivo temporário
    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_file:
        arquivo_temporario = temp_file.name
        plt.savefig(temp_file)

    return arquivo_temporario

# Plotando os resultados
plot_clusters(dfteste["frase"],"vida")

"""# 10.0 Gradio

"""

# Criação de dashBoard referente a analize de sentimentos, para ser exibido no gradio
def sentimento_dash(file, nome_da_coluna_texto, nome_da_coluna_datas):
    df = pd.read_csv(file.name)
    df_frequencia = df
    nome_da_coluna = nome_da_coluna_texto


    df['sentimento'] = pd.Series(prever1(df[nome_da_coluna_texto]))
    df['sentimento'] = df['sentimento'].replace({0: 'POSITIVE',1: 'NEUTRAL', 2: 'NEGATIVE'})

    #lista dos textos
    df['Primeiras 5 frases'] = df[nome_da_coluna_texto] + '  -  [' + df['sentimento'] + ']'
    texto = pd.DataFrame(df['Primeiras 5 frases']).head()



    #criacao do grafico
    quantidades = df['sentimento'].value_counts()
    labels = quantidades.index
    fig, ax = plt.subplots()
    fig.patch.set_facecolor('#FFFF') #mudar a cor de fundo do grafico

    ax.pie(quantidades, labels=labels, autopct='%1.1f%%')
    ax.set_title('Sentimentos do corpus')

    x = str(quantidades)

    #tamanho do corpus do corpus
    num_tuplas = "Número de Tuplas: " + str(len(df))


    fig2 = prepare_and_plot(df, nome_da_coluna_datas)


    return [num_tuplas,x, fig,fig2]

# Criação dos dados do dashboard referente à analize das palavras mais frequentes
def frequencia_dash(file, nome_da_coluna_texto, referencia):
    df = pd.read_csv(file.name)

    #frequencia
    frequencia = frequentes(pipeline(df[nome_da_coluna_texto]))

    # Armazena o gráfico de barras na variável graficofrequencia
    graficofrequencia = gerarGraficoFrequencia(frequencia)

    nuvemParaPlotar = exibirNuvemPalavras(pipeline(df[nome_da_coluna_texto]))

    clusters = plot_clusters(pipeline(df[nome_da_coluna_texto]), referencia)
    return graficofrequencia, nuvemParaPlotar, clusters

tituloDescricao = "# Rede Neural com Sentence Transformers"
textoDescricao1 = "A Rede Neural Recorrente (RNN) e a arquitetura Transformer foram combinadas em um esforço para entender melhor e analisar textos, tendo em vista a capacidade da RNN de lidar com sequências e a competência do Transformer para compreender as relações entre palavras em sentenças. Além disso, a introdução de uma segunda rede neural para a classificação de sentimentos permitiu uma análise mais profunda e contextualizada do conteúdo, visando identificar sentimentos como positivo, negativo ou neutro."
textoDescricao2 = "Essa combinação complexa de RNN, Transformer e uma segunda rede neural para análise de sentimentos demonstrou resultados positivos tanto quantitativos quanto qualitativos, permitindo um entendimento mais aprofundado e uma classificação mais precisa de sentimentos nos textos analisados. A riqueza dos insights gerados pelo uso dessas técnicas de processamento de linguagem natural modernas fortalece o potencial de sua aplicação em futuras análises de sentimentos."

tituloPagina1 = "# Análise de Sentimento"
textoPagina1 = "##### Complete os Três Passos para obter uma Análise de Sentimento do seu Arquivo"

tituloPagina2 = "# Análise de Frequência"
textoPagina2 = "##### Realize Dois Passos para Obter uma Análise de Frequência do Seu Arquivo"

#Iniciando gradio
with gr.Blocks(theme = "Soft", title = "Análise de Sentimento") as iface1:
    with gr.Tab("Sentimento"):
      gr.Markdown(tituloPagina1)
      gr.Markdown(textoPagina1)
      text_input = [gr.File(label = "Arquivo CSV"),gr.Textbox(label = "Nome da coluna dos textos", placeholder="Ex: ''Texto''"),gr.Textbox(label = "Nome da coluna das datas", placeholder="Ex: ''Data Publicada''")]
      sentimento_button = gr.Button("Aplicar")
      text_output = [gr.Textbox(label = "Numero de Linhas do Arquivo"),gr.Textbox(label = "Sentimento do corpus"),gr.Plot(label = "Dispersão de sentimento"),gr.Plot(label = "Grafico Sentimento - Mês")]
    with gr.Tab("Frequência"):
        gr.Markdown(tituloPagina2)
        gr.Markdown(textoPagina2)
        image_input = [gr.File(label = "Arquivo CSV"),gr.Textbox(label = "Nome da coluna dos textos", placeholder="Ex:''Texto''"),gr.Textbox()]
        frequência_button = gr.Button("Aplicar")
        image_output = [gr.Plot(label = "Frequência das Palavras"), gr.Image(label = "Nuvem de Palavras"),gr.Image(label = "Cluster")]
    with gr.Tab("Detalhes do Modelo"):
        gr.Markdown(tituloDescricao)
        gr.Markdown(textoDescricao1)
        gr.Markdown(textoDescricao2)
        testar_input = [gr.Textbox(label = "Teste o Modelo",placeholder= "Insira uma sentenca")]
        testar_button = gr.Button("Testar")
        testar_output = [gr.Textbox(label = "Resultado do Teste")]

    sentimento_button.click(sentimento_dash, inputs=text_input, outputs=text_output)
    frequência_button.click(frequencia_dash,inputs=image_input, outputs=image_output)
    testar_button.click(testar_frase,inputs=testar_input, outputs=testar_output)


iface1.launch()